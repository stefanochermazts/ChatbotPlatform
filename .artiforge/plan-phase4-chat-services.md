# 🚀 Phase 4: Chat Services Refactoring - Implementation Plan

**Generated by**: Artiforge AI  
**Date**: 14 Ottobre 2025  
**Target**: ChatCompletionsController (789 LOC → ~80-100 LOC)  
**Estimated Effort**: 10-12 hours  

---

## 🎯 Objective

Refactor the `ChatCompletionsController` (789 LOC) following the same SOLID principles applied in Phase 3 (Ingestion Pipeline). Extract business logic into 4 specialized Services while maintaining 100% backward compatibility with OpenAI Chat Completions API.

---

## 📊 Context

### Current State
- **ChatCompletionsController**: 789 LOC, 13 methods (1 public + 12 private)
- **Dependencies**: 7 services already injected (OpenAIChatService, KbSearchService, etc.)
- **Responsibilities**: RAG orchestration, citation scoring, fallback logic, profiling, streaming
- **Complexity**: Business logic in private methods, complex error handling

### Target State
- **ChatCompletionsController**: ~80-100 LOC (thin controller, delegation only)
- **4 New Services**: ChatOrchestrationService, ContextScoringService, FallbackStrategyService, ChatProfilingService
- **Single Responsibility**: Each Service with clear, focused responsibility
- **100% Testable**: All dependencies injectable and mockable
- **Backward Compatible**: Exact same API response format

---

## 🏗️ Services to Implement

### 1. ChatOrchestrationService (~300 LOC)
**Responsibility**: Orchestrate entire RAG pipeline

**Key Functions**:
- KB selection logic
- Intent detection coordination
- Context building coordination
- LLM generation coordination
- Citation handling
- Streaming response handling

### 2. ContextScoringService (~150 LOC)
**Responsibility**: Calculate and rank citation scores

**Key Functions**:
- Calculate smart source score
- Calculate content quality score
- Calculate source authority score
- Calculate intent match score
- Rank citations by composite scores
- Apply min_confidence threshold

### 3. FallbackStrategyService (~50 LOC)
**Responsibility**: Handle errors and implement fallback strategies

**Key Functions**:
- Handle LLM timeouts
- Handle no results scenarios
- Retry with exponential backoff
- Cached response fallback
- Generic fallback messages
- Error logging and tracking

### 4. ChatProfilingService (~50 LOC)
**Responsibility**: Track performance metrics

**Key Functions**:
- Track request latency (per step)
- Log metrics to Redis/Database
- Track token usage and costs
- Calculate quality metrics
- Generate correlation IDs
- Alert on performance issues

---

## 📋 Implementation Steps

### Step 1: Create Contract Interfaces ⏱️ 30 min

**Action**: Create contract interfaces for the four new services.

**Files to Create**:
- `app/Contracts/Chat/ChatOrchestrationServiceInterface.php`
- `app/Contracts/Chat/ContextScoringServiceInterface.php`
- `app/Contracts/Chat/FallbackStrategyServiceInterface.php`
- `app/Contracts/Chat/ChatProfilingServiceInterface.php`

**Requirements**:
- Use `declare(strict_types=1)`
- PHP 8.2+ typed parameters and return types
- Comprehensive docblocks
- Single public method per interface:
  - `orchestrate(array $request): Generator|Response` (orchestration)
  - `scoreCitations(array $citations, array $context): array` (scoring)
  - `handleFallback(array $request, \Throwable $exception): Response` (fallback)
  - `profile(array $metrics): void` (profiling)

**Testing**:
- Run `php artisan pint` to verify PSR-12 compliance

---

### Step 2: Create Chat Exception Class ⏱️ 20 min

**Action**: Create domain-specific exception class for chat-related errors.

**File to Create**:
- `app/Exceptions/ChatException.php`

**Requirements**:
- Extend `\Exception`
- Static factory methods (e.g., `fromTimeout()`, `fromInvalidResponse()`)
- `public int $statusCode` property for HTTP mapping
- JSON-serializable `toArray()` method (no stack trace)
- Comprehensive docblock

**Testing**:
- Pest test asserting factory methods return correct instances with expected status codes

---

### Step 3: Implement ContextScoringService ⏱️ 2 hours

**Action**: Implement `ContextScoringService` adhering to its contract.

**File to Create**:
- `app/Services/Chat/ContextScoringService.php`

**Dependencies to Inject**:
- `ConfigRepository $config` (read `rag.min_confidence`)
- `LoggerInterface $logger` (optional, for debug logs)

**Implementation Details**:
1. Compute four scores using private helper methods:
   - Source score
   - Quality score
   - Authority score
   - Intent match score
2. Combine into composite score (weighted sum, configurable weights)
3. Filter citations below `min_confidence`
4. Return sorted citations (descending by composite score)
5. Log each scoring step at debug level (JSON context)

**Error Handling**:
- Throw `ChatException::fromInvalidResponse()` if required fields missing
- Guard against division-by-zero in weighted averages

**Testing**:
- Correct weighting calculation
- Filtering below min_confidence
- Handling of empty citation list
- Exception when citation schema invalid

---

### Step 4: Implement FallbackStrategyService ⏱️ 1.5 hours

**Action**: Implement `FallbackStrategyService` adhering to its contract.

**File to Create**:
- `app/Services/Chat/FallbackStrategyService.php`

**Dependencies to Inject**:
- `CacheRepository $cache` (Redis-backed)
- `LoggerInterface $logger`
- `int $maxRetries = 3` (configurable)
- `int $baseDelayMs = 200` (exponential backoff base)

**Implementation Details**:
1. Detect timeout or empty-result scenario
2. Attempt `$maxRetries` with exponential backoff: `usleep($baseDelayMs * 2 ** $attempt)`
3. On final failure, check cached fallback response
4. If no cache, return default "I'm sorry..." message
5. Log each retry attempt with elapsed time
6. Ensure OpenAI Chat Completion format

**Error Handling**:
- Catch cache driver exceptions → fallback to generic message
- Guard against infinite loops (enforce retry limit)

**Testing**:
- Successful retry path
- Exhausted retries → cached fallback
- No cache → default generic message
- Proper exponential backoff timing

---

### Step 5: Implement ChatProfilingService ⏱️ 1.5 hours

**Action**: Implement `ChatProfilingService` adhering to its contract.

**File to Create**:
- `app/Services/Chat/ChatProfilingService.php`

**Dependencies to Inject**:
- `RedisConnection $redis` (or `CacheRepository`)
- `LoggerInterface $logger`
- `string $serviceName = 'chat'`

**Implementation Details**:
1. Accept metrics with keys: `step`, `duration_ms`, `tokens_used`, `cost_usd`, `correlation_id`
2. Push JSON line to Redis stream `chat:profiling`
3. Increment per-step latency counters (`INCRBY chat:latency:{step}`)
4. Log warning if `duration_ms` exceeds threshold (configurable)
5. Generate correlation_id if not supplied (UUID v4)
6. JSON-encoded logs with consistent schema

**Error Handling**:
- If Redis unavailable, fallback to file-based logging

**Testing**:
- Correct Redis stream entry format
- Counter increments
- Alert logging when threshold exceeded
- Graceful degradation when Redis throws exception

---

### Step 6: Implement ChatOrchestrationService ⏱️ 4 hours

**Action**: Implement `ChatOrchestrationService` adhering to its contract.

**File to Create**:
- `app/Services/Chat/ChatOrchestrationService.php`

**Dependencies to Inject**:
- `OpenAIChatService $llm`
- `KbSearchService $kbSearch`
- `ContextBuilder $contextBuilder`
- `ConversationContextEnhancer $contextEnhancer`
- `TenantRagConfigService $configService`
- `ContextScoringServiceInterface $scoringService`
- `FallbackStrategyServiceInterface $fallbackService`
- `ChatProfilingServiceInterface $profilingService`
- `LoggerInterface $logger`

**Implementation Details**:
1. Generate `$correlationId = (string) Str::uuid()`
2. Profile start of "orchestration" step
3. Retrieve tenant-specific RAG config
4. Perform KB selection (vector + BM25)
5. Build context with `$contextBuilder`
6. Enhance conversation context
7. Profile "context_build" step
8. Score citations via `$scoringService`
9. Attach citations to LLM payload
10. If streaming: `$llm->streamChat()` → yield chunks, profile "llm_stream"
11. If not streaming: `$llm->chat()` → return response, profile "llm_sync"
12. Profile total latency
13. Return OpenAI-formatted response
14. Wrap in try/catch → delegate to `$fallbackService` on exception

**Error Handling**:
- Catch `OpenAIException` → treat as LLM failure
- Catch `ChatException` → re-throw
- Generic `\Throwable` → wrap in `ChatException::fromInvalidResponse()`
- Never leak internal details to client

**Testing**:
- Successful end-to-end flow (non-streaming)
- Streaming flow (assert generator yields correctly)
- Failure in KB search → triggers fallback
- Failure in LLM call → triggers fallback
- Profiling metrics recorded
- Correlation ID propagated

---

### Step 7: Refactor ChatCompletionsController ⏱️ 1 hour

**Action**: Refactor `ChatCompletionsController` to a thin delegator.

**File to Modify**:
- `app/Http/Controllers/Api/ChatCompletionsController.php`

**Changes**:
1. Keep existing route signature and request validation
2. Constructor inject only `ChatOrchestrationServiceInterface $orchestrator`
3. Remove direct usage of 7 original services
4. Method `public function __invoke(Request $request)`:
   - Validate request payload (OpenAI schema)
   - Call `$response = $this->orchestrator->orchestrate($validated)`
   - If streaming: return `StreamedResponse` that echoes chunks
   - Otherwise: return `response()->json($response)`
   - Append rate-limit headers
   - Catch `ChatException` → transform to JSON error (no stack trace)
5. Remove all private helper methods
6. Add comprehensive docblock

**Target**: **~80-100 LOC** (currently 789 LOC)

**Error Handling**:
- Catch `ChatException` → `response()->json($e->toArray(), $e->statusCode)`
- Unexpected `\Throwable` → log + return generic 500 error

**Testing**:
- Successful non-streaming request → 200 + correct JSON
- Successful streaming request → `StreamedResponse` with SSE headers
- Validation errors → 422 + OpenAI-style error
- Service `ChatException` → correct status code
- Rate-limit headers present

---

### Step 8: Register Service Bindings ⏱️ 15 min

**Action**: Register service bindings in `AppServiceProvider`.

**File to Modify**:
- `app/Providers/AppServiceProvider.php`

**Changes**:
Inside `register()` add:
```php
$this->app->bind(
    \App\Contracts\Chat\ChatOrchestrationServiceInterface::class,
    \App\Services\Chat\ChatOrchestrationService::class
);
$this->app->bind(
    \App\Contracts\Chat\ContextScoringServiceInterface::class,
    \App\Services\Chat\ContextScoringService::class
);
$this->app->bind(
    \App\Contracts\Chat\FallbackStrategyServiceInterface::class,
    \App\Services\Chat\FallbackStrategyService::class
);
$this->app->bind(
    \App\Contracts\Chat\ChatProfilingServiceInterface::class,
    \App\Services\Chat\ChatProfilingService::class
);
```

**Post-Change**:
- Run `php artisan config:cache`

**Testing**:
- Integration test resolving each interface from container
- Assert instance is of expected concrete class

---

### Step 9: Write Comprehensive Tests ⏱️ 3 hours

**Action**: Write unit and integration tests for all services and controller.

**Files to Create**:
- `tests/Unit/Services/Chat/ContextScoringServiceTest.php`
- `tests/Unit/Services/Chat/FallbackStrategyServiceTest.php`
- `tests/Unit/Services/Chat/ChatProfilingServiceTest.php`
- `tests/Unit/Services/Chat/ChatOrchestrationServiceTest.php`
- `tests/Feature/ChatCompletionsControllerTest.php`

**Testing Strategy**:
- Use Pest and Mockery to stub external dependencies
- Mock OpenAIChatService, KbSearchService, CacheRepository, RedisConnection
- Test scenarios:
  - Normal flow (happy path) for each service
  - Edge cases (empty citations, missing fields, timeout, cache hit/miss)
  - Exception handling paths → `ChatException`
  - Orchestrator respects `stream` flag
  - Profiling metrics recorded
  - Fallback service respects exponential backoff
- Controller tests:
  - Successful non-streaming request
  - Successful streaming request
  - Validation error
  - Service-level error → fallback response
  - Rate-limit headers present

**Coverage Target**: >85%

**Command**: `php artisan test --coverage`

---

### Step 10: Update Documentation ⏱️ 1 hour

**Action**: Update documentation and create Artiforge change-log.

**Files to Create/Update**:
- `docs/chat-services.md` (new)
- `.artiforge/phase4-refactor-chat-services.md` (new)
- `README.md` (update)

**Documentation Content**:
- High-level diagram (mermaid flowchart)
- Migration notes (no API changes, internal refactor only)
- Configuration guide (scoring weights, fallback cache TTL, profiling thresholds)
- Extension instructions (mocking services)
- Artiforge file with:
  - Summary of changes
  - List of new/modified files
  - Reasoning (SOLID, testability, performance)
  - Verification steps

**Commit Message**: 
```
refactor(chat): split ChatCompletionsController into dedicated services (Phase 4)
```

---

### Step 11: Performance Smoke-Test ⏱️ 1 hour

**Action**: Perform performance smoke-test in staging.

**Requirements**:
- Deploy to staging instance
- Use load-testing tool (e.g., `hey` or `k6`)
- Test mix of streaming and non-streaming requests (≈200 concurrent users)

**Metrics to Capture**:
- Average and P95 latency
- Error rate
- Redis profiling stream volume
- CPU/Memory usage

**Success Criteria**:
- P95 latency < 2.5s
- Error rate < 1%
- No memory leaks
- CPU usage stable

**If Issues Arise**:
- Investigate profiling data
- Consider caching citation scores
- Adjust scoring weight calculations

**Documentation**:
- Attach results to Artiforge markdown
- Include logs in merge request

---

## 📊 Expected Results

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **LOC** | 789 | ~100 | **-87%** |
| **Methods** | 13 | 1-2 | **-85%** |
| **Services** | 7 injected | 1 injected | **-86%** |
| **Maintainability** | 25/100 | 80/100 | **+220%** |
| **Complexity** | 160 | 20 | **-87%** |
| **Testability** | Low | High | **+400%** |

---

## ✅ Critical Requirements

- ✅ Maintain OpenAI API compatibility (exact response format)
- ✅ Preserve streaming response functionality (SSE)
- ✅ No breaking changes to existing behavior
- ✅ All Services implement interfaces
- ✅ All Services unit testable
- ✅ Preserve existing Services (reuse, don't replace)
- ✅ Production-test before committing

---

## 🎯 Success Criteria

- [ ] ChatCompletionsController reduced to ~80-100 LOC
- [ ] 4 Services implemented with interfaces
- [ ] 0 linter errors (PSR-12 compliant)
- [ ] Test coverage >85%
- [ ] Production smoke-test passed (P95 < 2.5s)
- [ ] OpenAI API compatibility verified
- [ ] Streaming responses working
- [ ] No breaking changes to widget
- [ ] Documentation updated
- [ ] Commit & push successful

---

## 📚 Lessons from Phase 3

### ✅ What Worked Well
1. Interface-first approach
2. Incremental steps
3. Production testing early
4. Comprehensive documentation
5. Backward compatibility focus

### ⚠️ Watch Out For
1. API signature mismatches
2. Return type mismatches
3. Parameter format differences
4. Streaming complexity
5. Error handling preservation

---

## ⏰ Time Estimate

| Phase | Time |
|-------|------|
| Steps 1-2 (Interfaces + Exception) | 1h |
| Steps 3-5 (3 Simple Services) | 5h |
| Step 6 (Orchestration Service) | 4h |
| Step 7 (Controller Refactor) | 1h |
| Steps 8-9 (Bindings + Tests) | 3h |
| Steps 10-11 (Docs + Smoke Test) | 2h |
| **Total** | **16h** |

**Note**: Initial estimate was 10-12h, but detailed plan suggests 16h is more realistic for high-quality implementation with comprehensive testing.

---

## 🚀 Ready to Start!

**Next Action**: Proceed with **Step 1** (Create Contract Interfaces)


